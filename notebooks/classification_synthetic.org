#+SETUPFILE: ~/life/references/templates/org/literate_programming_setupfile.org
#+SETUPFILE: ~/life/references/templates/org/maths_setupfile.org
#+PROPERTY: header-args :kernel meta_learning :tangle yes :resuls output
#+LATEX_HEADER_EXTRA: \setmainfont{Libre Baskerville}

* Introduction
:LOGBOOK:
CLOCK: [2020-05-14 Thu 17:46]--[2020-05-14 Thu 18:11] =>  0:25
CLOCK: [2020-05-14 Thu 17:16]--[2020-05-14 Thu 17:38] =>  0:22
CLOCK: [2020-05-12 Tue 15:51]--[2020-05-12 Tue 16:16] =>  0:25
CLOCK: [2020-05-12 Tue 13:27]--[2020-05-12 Tue 13:52] =>  0:25
CLOCK: [2020-05-12 Tue 11:57]--[2020-05-12 Tue 12:22] =>  0:25
:END:
Look at how the KNN works on a misspecified dataset, we have 2nd order datasets
going through the origin.

Import necessary libraries
#+begin_src jupyter-python
import os
from pathlib import Path
import re

from loguru import logger
import numpy as np
from scipy.spatial.distance import pdist, squareform
from sklearn.decomposition import KernelPCA
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import ParameterGrid
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
import hickle as kl
from torch.utils.data import DataLoader

# AML imports
from active_meta_learning.data import (
    EnvironmentDataSet,
    UniformSphere,
    HypercubeWithKVertexGaussian,
    VonMisesFisherMixture,
)
from active_meta_learning.data_utils import (
    aggregate_sampled_task_batches,
    coalesce_train_and_test_in_dicts,
    convert_batches_to_fw_form,
    convert_batches_to_np,
    get_task_parameters,
    remove_batched_dimension_in_D,
    reorder_list,
    set_random_seeds,
    form_datasets_from_tasks,
    npfy_batches,
)
from active_meta_learning.kernels import (
    gaussian_kernel_matrix,
    gaussian_kernel_mmd2_matrix,
    median_heuristic,
    mmd2,
)
from active_meta_learning.optimisation import KernelHerding
from active_meta_learning.estimators import (
    RidgeRegression,
    BiasedRidgeRegression,
    RidgeRegPrototypeEstimator,
    TrueWeightPrototypeEstimator,
    GDLeastSquares,
)
#+end_src

#+RESULTS:

* Functions
Additional functions which will be use
** Plotting
#+begin_src jupyter-python
def plot_task_weights(tasks, ax):
    ws = get_task_parameters(tasks)
    ax.scatter(ws[:, 0], ws[:, 1])
    return ax

def plot_kh_weights(kh_w, kh_D, train_tasks, until_t=10):
    ws = get_task_parameters(train_tasks)
    ws = KernelPCA(n_components=2, kernel="rbf").fit_transform(ws)
    fig, ax = plt.subplots(1, 3, figsize=(4 * 3, 4))
    bbox = dict(boxstyle="round", fc="0.8")
    # kh_w
    ax[0].scatter(ws[:, 0], ws[:, 1], alpha=0.2, color="black")
    ws_w = ws[kh_w.sampled_order[:until_t]]
    ax[0].scatter(ws_w[:until_t, 0], ws_w[:until_t, 1], color="red")
    for i in range(until_t):
        ax[0].annotate(i+1, (ws_w[i, 0], ws_w[i, 1]), bbox=bbox, size=10)
    ax[0].set_title("First {} chosen point (KH weights)".format(until_t))

    # kh_D
    ax[1].scatter(ws[:, 0], ws[:, 1], alpha=0.2, color="black")
    ws_D = ws[kh_D.sampled_order[:until_t]]
    ax[1].scatter(ws_D[:until_t, 0], ws_D[:until_t, 1], color="red")
    for i in range(until_t):
        ax[1].annotate(i+1, (ws_D[i, 0], ws_D[i, 1]), bbox=bbox, size=10)
    ax[1].set_title("First {} chosen point (KH data)".format(until_t))

    # No reordering
    ax[2].scatter(ws[:, 0], ws[:, 1], alpha=0.2, color="black")
    ws_u = ws[np.random.permutation(ws.shape[0])[:until_t]]
    ax[2].scatter(ws_u[:until_t, 0], ws_u[:until_t, 1], color="red")
    for i in range(until_t):
        ax[2].annotate(i+1, (ws_u[i, 0], ws_u[i, 1]), bbox=bbox, size=10)
    ax[2].set_title("First {} chosen point (Random)".format(until_t))
    plt.tight_layout()

    return ax

def gen_3d_ax():
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    return fig, ax

def plot_task_lin_reg(task, ax):
    X_tr, y_tr = task["train"]
    X_tr = KernelPCA(n_components=2, kernel="rbf").fit_transform(X_tr)
    X_te, y_te = task["test"]
    X_te = KernelPCA(n_components=2, kernel="rbf").fit_transform(X_te)
    # train
    ax.scatter(X_tr[:, 0], X_tr[:, 1], y_tr, c="blue", label="train set")
    # test
    ax.scatter(X_te[:, 0], X_te[:, 1], y_te, c="red", label="test set")
    ax.legend()
    return ax

def plot_aml_ci(ax, error, color, label, until_t):
    mean = error.mean(axis=1)
    std = np.std(error, axis=1)
    ax.plot(mean[:until_t], label=label, color=color)
    upper_ci = mean + std
    lower_ci = mean - std
    ax.fill_between(
        np.arange(until_t),
        lower_ci[:until_t],
        upper_ci[:until_t],
        color=color,
        alpha=0.2,
    )

def plot_itl_ci(ax, error, color, label, until_t):
    mean = np.mean(error)
    std = np.std(error)
    ax.axhline(mean, label=label, color=color, linestyle="--")
    upper_ci = mean + std
    lower_ci = mean - std
    ax.fill_between(
        np.arange(until_t), lower_ci, upper_ci, color=color, alpha=0.2
    )

def plot_hist_with_ci(data, ax, n_std=1):
    mean = np.mean(data)
    std = np.std(data)
    lci = mean - n_std * std
    uci = mean + n_std * std
    ax.hist(data)
    ax.axvline(mean, color="red")
    ax.axvline(lci, color="red", alpha=0.4)
    ax.axvline(uci, color="red", alpha=0.4)

def get_ci(matrix):
    # row is time, columns are runs
    mean = np.mean(matrix, axis=1)
    std = np.std(matrix, axis=1)
    return mean, mean - std, mean + std
#+end_src

#+RESULTS:

** Data
#+begin_src jupyter-python
def y_func_2nd_order(X, w):
    return (X @ w) * (X @ w + 1)

def generate_data(env, noise_w, noise_y, k_shot, k_query, num_train_tasks=100, num_val_tasks=100, num_test_tasks=100, y_func=y_func_2nd_order):
    env_dataset = EnvironmentDataSet(k_shot, k_query, env, noise_w, noise_y, y_func)
    dataloader = DataLoader(
        env_dataset, collate_fn=env_dataset.collate_fn, batch_size=1,
    )
    # Train data
    train_tasks = aggregate_sampled_task_batches(dataloader, num_train_tasks)
    train_tasks_kh = convert_batches_to_fw_form(train_tasks)
    train_tasks = npfy_batches(train_tasks)

    # Val data
    val_tasks = aggregate_sampled_task_batches(dataloader, num_val_tasks)
    val_tasks_kh = convert_batches_to_fw_form(val_tasks)
    val_tasks = npfy_batches(val_tasks)

    # Test data
    test_tasks = aggregate_sampled_task_batches(dataloader, num_test_tasks)
    test_tasks_kh = convert_batches_to_fw_form(test_tasks)
    test_tasks = npfy_batches(test_tasks)
    return train_tasks, train_tasks_kh, val_tasks, val_tasks_kh, test_tasks, test_tasks_kh

def _mmd2_matrix(A, B, base_s2):
    assert len(A.shape) == 3
    m, n, d = A.shape
    assert len(B.shape) == 3
    p, q, l = B.shape
    M2 = np.zeros((m, p))
    for i in range(m):
        for j in range(p):
            K_xx = gaussian_kernel_matrix(A[i], s2=base_s2)
            K_yy = gaussian_kernel_matrix(B[j], s2=base_s2)
            K_xy = gaussian_kernel_matrix(A[i], B[j], s2=base_s2)
            M2[i, j] = mmd2(K_xx, K_yy, K_xy)
    return M2

def _gaussian_kernel_mmd2_matrix(A, B, base_s2, meta_s2):
    """Calculate the double gaussian kernel

    Calculate the double gaussian kernel matrix between A and B
    using base_s2 for the inner and meta_s2 for the outer level
    """
    M2 = _mmd2_matrix(A, B, base_s2)
    return np.exp(-0.5 * M2 / meta_s2)

def calculate_double_gaussian_median_heuristics(
    A, n_base_subsamples=None, n_meta_subsamples=None
):
    """A.shape = (m, n, d), m is number of datasets, n is the size, d is the dimension"""
    assert len(A.shape) == 3
    m, n, d = A.shape
    if n_base_subsamples is None:
        vec_A = A.reshape(-1, d)
        pairwise_square_dists = squareform(pdist(vec_A, "sqeuclidean"))
    else:
        vec_A = A.reshape(-1, d)
        subsample_indices = np.random.permutation(vec_A.shape[0])[:n_base_subsamples]
        vec_A = vec_A[subsample_indices]
        pairwise_square_dists = squareform(pdist(vec_A, "sqeuclidean"))
    base_s2 = median_heuristic(pairwise_square_dists)
    M2 = np.zeros((m, m))
    for i in range(m):
        for j in range(i):
            K_xx = gaussian_kernel_matrix(A[i], s2=base_s2)
            # K_xx = K_xx + np.eye(n)*eps
            K_yy = gaussian_kernel_matrix(A[j], s2=base_s2)
            # K_yy = K_yy + np.eye(n)*eps
            K_xy = gaussian_kernel_matrix(A[i], A[j], s2=base_s2)
            M2[i, j] = mmd2(K_xx, K_yy, K_xy)
    # Only have lower diagonal entries and diag=0
    # this way we avoid computing m(m-1)/2 entries
    M2 = M2 + M2.T
    meta_s2 = median_heuristic(M2, n_meta_subsamples)

    return base_s2, meta_s2
#+end_src

#+RESULTS:

** Meta-learning
:LOGBOOK:
CLOCK: [2020-05-13 Wed 15:37]--[2020-05-13 Wed 16:02] =>  0:25
:END:
#+begin_src jupyter-python
class IndependentTaskLearning:
    def __init__(self, tasks, algorithm, loss=mean_squared_error):
        """
        :param tasks: tasks that ITL will be performed over by algorithm
        :type tasks: list of tasks, where each task is a dict with keys ("train", "test")
            and values (X_tr, y_tr), tuple of numpy arrays
        :param algorithm: algorithm implementing sklearn fit / predict framework
        :type algorithm: instance of predictor class with fit / predict defined
        :param loss: loss function taking loss(y, y_pred)
        :type loss: loss(y: np.ndarray, y_pred: np.ndarray) -> float
        """
        self.tasks = tasks
        self.algorithm = algorithm
        self.loss = loss

    def fit(self, task):
        """Fit `algorithm` to the i'th task"""
        X_tr, y_tr = task["train"]
        self.algorithm.fit(X_tr, y_tr)

    def predict(self, task):
        X_te, _ = task["test"]
        return self.algorithm.predict(X_te)

    def get_loss(self, task):
        _, y_te = task["test"]
        self.fit(task)
        y_pred = self.predict(task)
        return self.loss(y_pred, y_te)

    def calculate_transfer_risk(self):
        # Collect loss for each task in tasks
        self.losses_ = []
        self.weights_ = []
        for task in self.tasks:
            self.losses_.append(self.get_loss(task))
            self.weights_.append(self.algorithm.w_hat_)
        self.losses_ = np.array(self.losses_)
        self.weights_ = np.array(self.weights_)

    def set_params(self, params):
        """Update paramaters of algorithm"""
        self.algorithm.set_params(**params)

    def get_params(self):
        """Update paramaters of algorithm"""
        return self.algorithm.get_params()

def cross_validate_itl(itl, cv_params):
    """Cross validate itl over cv_params

    itl is an object of class IndependentTaskLearning
    and cv_params is a dictionary of the values for each
    parameter."""
    param_grid = ParameterGrid(cv_params)
    opt_loss = np.inf
    opt_params = None
    for params in param_grid:
        logger.info("Cross validating params: {}".format(params))
        itl.set_params(params)
        itl.calculate_transfer_risk()
        current_loss = np.mean(itl.losses_)
        logger.info("Current loss: {}".format(current_loss))
        if current_loss < opt_loss:
            opt_loss = current_loss
            opt_params = params
            logger.info(
                "Best params so far {}, with loss {:.4f}".format(params, opt_loss)
            )
    return opt_params, opt_loss


class MetaKNNExperiment:
    """Full experiment optimised for speed

    This does not adhere to the sklearn like
    fit/transform/predict framework."""

    def __init__(
        self,
        train_tasks,
        test_tasks,
        dist_tr_te,
        base_algorithm,
        k_nn,
        train_task_reordering,
        prototype_estimator,  # transformer prototype
    ):
        self.train_tasks = train_tasks
        self.test_tasks = test_tasks
        self.dist_tr_te = dist_tr_te
        self.base_algorithm = base_algorithm
        self.k_nn = k_nn
        self.train_task_reordering = train_task_reordering
        self.prototype_estimator = prototype_estimator
        self.d = self.train_tasks[0]["w"].shape[0]

        self.T_tr, self.T_te = len(train_tasks), len(test_tasks)
        assert self.dist_tr_te.shape == (self.T_tr, self.T_te)
        self._reorder()
        self._form_datasets_from_tasks()
        self.calculate_prototypes()

    def _reorder(self):
        self.train_tasks = [self.train_tasks[i] for i in self.train_task_reordering]
        self.dist_tr_te = self.dist_tr_te[
            np.ix_(self.train_task_reordering, np.arange(self.T_te))
        ]

    def _form_datasets_from_tasks(self):
        self.train_datasets = form_datasets_from_tasks(self.train_tasks)
        self.test_datasets = form_datasets_from_tasks(self.test_tasks)
        self.datasets = np.concatenate(
            [self.train_datasets, self.test_datasets], axis=0
        )

    def calculate_prototypes(self):
        """Recalculate prototypes using prototype_estimator

        This allows us to cross-validate after we change the prototype
        estimator parameters"""
        self.prototypes = self.prototype_estimator.transform(self.train_tasks)

    def _adapt(self, i, t):
        """Adapt to one task with index i when meta-train set is of size t"""
        test_task = self.test_tasks[i]
        X_tr, y_tr = test_task["train"]
        distances = self.dist_tr_te[:t, i]

        knn_prototypes = self.prototypes[np.argsort(distances)[: self.k_nn], :]

        w_0 = np.mean(knn_prototypes, axis=0)
        self.w_0_[t, i] = w_0
        self.base_algorithm.fit(X_tr, y_tr, w_0=w_0)
        self.w_hat_[t, i] = self.base_algorithm.w_hat_

    def _loss(self, i, t):
        test_task = self.test_tasks[i]
        X_te, y_te = test_task["test"]
        self._adapt(i, t)
        return mean_squared_error(y_te, self.base_algorithm.predict(X_te))

    def calculate_transfer_risk(self):
        """Calculate the transfer risk"""
        # The loss matrix is a matrix of size (T_tr, T_te)
        # Note that the first self.k_nn columns are nans selfince
        # we do not fill them in as not eno ugh prototypes are available
        self.loss_matrix_ = np.zeros((self.T_tr, self.T_te))
        self.loss_matrix_[: self.k_nn, :] = np.nan
        # Additional info
        self.w_0_ = np.zeros((self.T_tr, self.T_te, self.d))
        self.w_hat_ = np.zeros((self.T_tr, self.T_te, self.d))
        # Each t represents using meta-train instances up until t according to
        # ordering as prototypes. We need to start at k_nn to be able to find
        # k_nn neighbours
        for t in range(self.k_nn, self.T_tr):
            for i in range(self.T_te):
                self.loss_matrix_[t, i] = self._loss(i, t)

    def set_base_algorithm_params(self, params):
        self.base_algorithm.set_params(**params)

    def get_base_algorithm_params(self):
        return self.base_algorithm.get_params()

    def set_prototype_estimator_params(self, params):
        self.prototype_estimator.set_params(**params)

    def get_prototype_estimator_params(self):
        return self.prototype_estimator.get_params()

    def set_params(self, params):
        # Need to catch estimator having no params
        self.set_base_algorithm_params(params["base_algorithm"])
        self.set_prototype_estimator_params(params["prototype_estimator"])

    def get_params(self):
        return {
            "base_algorithm": self.get_base_algorithm_params(),
            "prototype_estimator": self.get_prototype_estimator_params(),
        }
#+end_src

#+RESULTS:

* Hypersphere
:LOGBOOK:
CLOCK: [2020-05-14 Thu 18:26]--[2020-05-14 Thu 18:51] =>  0:25
CLOCK: [2020-05-13 Wed 16:14]--[2020-05-13 Wed 16:39] =>  0:25
CLOCK: [2020-05-12 Tue 20:26]--[2020-05-12 Tue 20:51] =>  0:25
CLOCK: [2020-05-12 Tue 14:41]--[2020-05-12 Tue 15:06] =>  0:25
CLOCK: [2020-05-12 Tue 14:10]--[2020-05-12 Tue 14:35] =>  0:25
:END:
All the data generated on the hypersphere.

#+begin_src jupyter-python
y_func_misspec = y_func_2nd_order

def generate_hypersphere_data(
    d, noise_w, noise_y, k_shot, k_query, num_train_tasks=100, num_val_tasks=100, num_test_tasks=100
):
    env = UniformSphere(d)
    return generate_data(env, noise_w, noise_y, k_shot, k_query, num_train_tasks, num_val_tasks, num_test_tasks, y_func_misspec)

d = 5
noise_w, noise_y = 0.01, 0.0
k_shot, k_query = 5, 25
n_tr, n_val, n_te = 100, 80, 100
train_tasks, train_tasks_kh, val_tasks, val_tasks_kh, test_tasks, test_tasks_kh = generate_hypersphere_data(d, noise_w, noise_y, k_shot, k_query, n_tr, n_val, n_te)
train_datasets = form_datasets_from_tasks(train_tasks)
val_datasets = form_datasets_from_tasks(val_tasks)
test_datasets = form_datasets_from_tasks(test_tasks)

train_ws = get_task_parameters(train_tasks)
val_ws = get_task_parameters(val_tasks)
test_ws = get_task_parameters(test_tasks)
#+end_src

#+RESULTS:

Plotting the weights
#+begin_src jupyter-python
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
plot_task_weights(train_tasks, ax)
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f2b7c91bc50>
[[file:./.ob-jupyter/cbcada75af562f7b4e512a819fd0d566d5acac20.png]]
:END:

Plot of the actual linear regression datasets generated. We pick 3 random tasks
which differ
#+begin_src jupyter-python
fig = plt.figure(figsize=(8, 16))
axes = [fig.add_subplot(3, 1, i, projection="3d") for i in range(1, 4)]
rand_idx = np.random.permutation(20)
for i, ax in enumerate(axes):
    task = train_tasks[rand_idx[i]]
    plot_task_lin_reg(task, ax)
    w = task["w"]
    ax.set_title("w: ({:.2f}, {:.2f})".format(w[0], w[1]))
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 460
[[file:./.ob-jupyter/85f182d40417f2f3780f31d32bc90e36bd34feb2.png]]
:END:

From the above plot we can see how they differ, since we are assuming a linear
regression setup the points all lie on hyperspheres.

Now we consider how the different KH methods pick points.
#+begin_src jupyter-python
# KH on data
base_s2_D, meta_s2_D = calculate_double_gaussian_median_heuristics(
    train_datasets, n_base_subsamples=200
)
K_D = _gaussian_kernel_mmd2_matrix(
    A=train_datasets, B=train_datasets, base_s2=base_s2_D, meta_s2=meta_s2_D
)
# KH on weights
train_task_ws = get_task_parameters(train_tasks)
ws_dataset = train_task_ws[:, np.newaxis, :]
base_s2_w, meta_s2_w = calculate_double_gaussian_median_heuristics(
    ws_dataset, n_base_subsamples=200
)
K_w = _gaussian_kernel_mmd2_matrix(
    A=ws_dataset, B=ws_dataset, base_s2=base_s2_w, meta_s2=meta_s2_w
)
#+end_src

#+RESULTS:

Investigate how the kernel matrix look like.
#+begin_src jupyter-python
fig, ax = plt.subplots()
ax.imshow(K_D, vmin=0.0, vmax=1.0)
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.image.AxesImage at 0x7f2b804aa090>
#+attr_org: :width 252
[[file:./.ob-jupyter/990fc60d9fe3bfe15b9060e9468b85d3574e92ad.png]]
:END:

#+begin_src jupyter-python
fig, ax = plt.subplots()
ax.imshow(K_w, vmin=0.0, vmax=1.0)
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.image.AxesImage at 0x7f2b7c9c7490>
#+attr_org: :width 252
[[file:./.ob-jupyter/66efd54fe0c6e48d7cc08c2902c2ae0fa94aaf4d.png]]
:END:

#+begin_src jupyter-python
fig, ax = plt.subplots()
_ = ax.hist((K_w - K_D)[np.triu_indices(n_tr, k=1)], range=(-1.0, 1.0), density=True)
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 380
[[file:./.ob-jupyter/11bc62d2a9813953f3745d894f401832994a8514.png]]
:END:

Looks pretty nice above, the weights and data dataset kernel matrix seem to find
something reasonable as the difference is not too unlike.

Running the actual kernel herding algorithms
#+begin_src jupyter-python
kh_D = KernelHerding(K_D)
kh_D.run()
kh_w = KernelHerding(K_w)
kh_w.run()
#+end_src

#+RESULTS:


#+begin_src jupyter-python
plot_kh_weights(kh_w, kh_D, train_tasks, until_t=20)
#+end_src

#+RESULTS:
:RESULTS:
: array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f2b7c88ee50>,
:        <matplotlib.axes._subplots.AxesSubplot object at 0x7f2b7cf33910>,
:        <matplotlib.axes._subplots.AxesSubplot object at 0x7f2b7ca6d550>],
:       dtype=object)
[[file:./.ob-jupyter/82ea11e5a0b07be4ee9c5a8466755abcc08a4ee3.png]]
:END:

** KNN Meta learning
:LOGBOOK:
CLOCK: [2020-05-13 Wed 17:48]--[2020-05-13 Wed 18:13] =>  0:25
:END:
We run KNN meta learning on the dataset to see and understand what works where.
We consider
- Uniform
- KH on data
- KH on weights
and we use the true weights. Alternatively, we will shrink the true weights to
see how that impacts (and possibly make it better than using the true weights).
This is due to the observed phenomenon where the ridge weights perform better
than the true weights.

Calculate the mmd distances from all the train to test tasks. These are used
later when we use the KNN algorithm
#+begin_src jupyter-python
train_datasets = form_datasets_from_tasks(train_tasks)
# Don't cheat, can only use support/train set for meta-test and meta-val
# tr_val_datasets = form_datasets_from_tasks(val_tasks, use_only_train=True)
tr_test_datasets = form_datasets_from_tasks(test_tasks, use_only_train=True)
# M_tr_val_D = np.sqrt(_mmd2_matrix(train_datasets, tr_val_datasets, base_s2_D))
M_tr_te_D = np.sqrt(_mmd2_matrix(train_datasets, tr_test_datasets, base_s2_D))
#+end_src

#+RESULTS:

We have two things to set, \(\alpha_{RR}, \alpha_{prot}\) for the regularisation
parameter in Ridge Regression and Prototypes.
#+begin_src jupyter-python
alpha_rrs = np.geomspace(1e-6, 3, 10)
alpha_prot = 0.001
#+end_src

#+RESULTS:

** ITL Ridge Reg
Evaluate ridge regression in an independent task learning setting
#+begin_src jupyter-python
rr = RidgeRegression(alpha=0.0001)
itl_rr = IndependentTaskLearning(train_tasks, rr)
itl_rr_opt_params, itl_rr_cross_val_loss = cross_validate_itl(
    itl_rr, {"alpha": alpha_rrs.tolist()}
)
itl_rr.set_params(itl_rr_opt_params)
itl_rr.tasks = test_tasks
itl_rr.calculate_transfer_risk()
#+end_src

#+RESULTS:
#+begin_example
2020-05-14 18:30:17.305 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 1e-06}
2020-05-14 18:30:17.389 | INFO     | __main__:cross_validate_itl:63 - Current loss: 1.4830236835252741
2020-05-14 18:30:17.390 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 1e-06}, with loss 1.4830
2020-05-14 18:30:17.390 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 5.244210785953468e-06}
2020-05-14 18:30:17.453 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.8602169957043435
2020-05-14 18:30:17.454 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 5.244210785953468e-06}, with loss 0.8602
2020-05-14 18:30:17.454 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 2.7501746767510743e-05}
2020-05-14 18:30:17.542 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.6842430784016297
2020-05-14 18:30:17.546 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 2.7501746767510743e-05}, with loss 0.6842
2020-05-14 18:30:17.550 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.0001442249570307409}
2020-05-14 18:30:17.607 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.5001292826030155
2020-05-14 18:30:17.608 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.0001442249570307409}, with loss 0.5001
2020-05-14 18:30:17.609 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.0007563460752642876}
2020-05-14 18:30:17.686 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.2754695855127354
2020-05-14 18:30:17.686 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.0007563460752642876}, with loss 0.2755
2020-05-14 18:30:17.687 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.0039664382458145546}
2020-05-14 18:30:17.769 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.12895219674888547
2020-05-14 18:30:17.770 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.0039664382458145546}, with loss 0.1290
2020-05-14 18:30:17.771 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.02080083823051906}
2020-05-14 18:30:17.852 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.08651990171359067
2020-05-14 18:30:17.855 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.02080083823051906}, with loss 0.0865
2020-05-14 18:30:17.856 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.10908398020536153}
2020-05-14 18:30:17.954 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.08661123315834379
2020-05-14 18:30:17.962 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.5720593855676914}
2020-05-14 18:30:18.058 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.11642222168915288
2020-05-14 18:30:18.059 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 3.0}
2020-05-14 18:30:18.148 | INFO     | __main__:cross_validate_itl:63 - Current loss: 0.186592308684624
#+end_example

The train loss
#+begin_src jupyter-python
fig, ax = plt.subplots()
sns.distplot(itl_rr.losses_, kde=True, rug=True, ax=ax)
ax.axvline(itl_rr.losses_.mean(), color="red", linestyle="--")
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.lines.Line2D at 0x7f2b7c787e90>
#+attr_org: :width 380
[[file:./.ob-jupyter/ee1566cb07e510a09674fe106a3c776daf1aee48.png]]
:END:

** Meta-learning setup
:LOGBOOK:
CLOCK: [2020-05-13 Wed 18:49]--[2020-05-13 Wed 19:14] =>  0:25
:END:
Now we proceed to do active meta learning. We will reorder the sequences
according to KH on weights and data. As the actual meta learning algorithm we
will use the KNN on datasets and use the biased ridge regression. We will reuse
the optimal alpha from ITL with ridge reg. From here on we split into two cases,
where for the prototype estimator we use
- Ridge Regression
- True Weights

we use 3 for the number of nearest neighbours
#+begin_src jupyter-python
k_nn = 3
#+end_src

#+RESULTS:

*** Biased Ridge Reg
:LOGBOOK:
CLOCK: [2020-05-13 Wed 19:34]--[2020-05-13 Wed 19:59] =>  0:25
:END:
#+begin_src jupyter-python
aml_order = kh_D.sampled_order
biased_rr = BiasedRidgeRegression(alpha=itl_rr_opt_params["alpha"])
model = MetaKNNExperiment(
    train_tasks,
    test_tasks,
    M_tr_te_D,
    biased_rr,
    k_nn,
    aml_order,
    RidgeRegPrototypeEstimator(alpha=alpha_prot),
)
model.calculate_transfer_risk()
#+end_src

#+RESULTS:

Let's look at how this evolved over time, and compare it with the ITL baseline
#+begin_src jupyter-python
fig, ax = plt.subplots()
t = np.arange(model.loss_matrix_.shape[0])
plot_aml_ci(ax, model.loss_matrix_, "blue", "aml", 100)
plot_itl_ci(ax, itl_rr.losses_, "red", "itl", 100)
ax.axhline(noise_y, color="black")
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.lines.Line2D at 0x7f2b7c6ed590>
#+attr_org: :width 387
[[file:./.ob-jupyter/62997d4dc1ce013c8545565cc0aa0dd24c7759a2.png]]
:END:

#+begin_src jupyter-python
zero_vs_ws = np.linalg.norm(test_ws, axis=1)
itl_vs_ws = np.linalg.norm(itl_rr.weights_ - test_ws, axis=1)
aml_w0_vs_ws = np.linalg.norm(model.w_0_[-1, :] - test_ws, axis=1)
aml_w_hat_vs_ws = np.linalg.norm(model.w_hat_[-1, :] - test_ws, axis=1)

fig, ax = plt.subplots(4, 1, sharex=True)
plot_hist_with_ci(zero_vs_ws, ax[0])
plot_hist_with_ci(itl_vs_ws, ax[1])
plot_hist_with_ci(aml_w0_vs_ws, ax[2])
plot_hist_with_ci(aml_w_hat_vs_ws, ax[3])
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 370
[[file:./.ob-jupyter/cadecc5fc879bba891abe7e8502559a0fe63c78b.png]]
:END:

I want to test the hypothesis of shrinkage. Let's compare the sizes of the
resulting \(w_{0}\) and \(\hat{w}\)
#+begin_src jupyter-python
fig, ax = plt.subplots()
ax.hist(np.linalg.norm(model.prototypes, axis=1))
#+end_src

#+RESULTS:
:RESULTS:
| array | ((24 20 15 11 14 11 3 2)) | array | ((0.13777795 0.48335046 0.82892297 1.17449547 1.52006798 1.86564049 2.211213 2.5567855 2.90235801)) | <a | list | of | 8 | Patch | objects> |
#+attr_org: :width 370
[[file:./.ob-jupyter/ef37b030c46e89177d623945228ecb5a00524f87.png]]
:END:

*** True weights
:LOGBOOK:
CLOCK: [2020-05-13 Wed 19:34]--[2020-05-13 Wed 19:59] =>  0:25
:END:
#+begin_src jupyter-python
aml_order = kh_D.sampled_order
biased_rr = BiasedRidgeRegression(alpha=itl_rr_opt_params["alpha"])
model = MetaKNNExperiment(
    train_tasks,
    test_tasks,
    M_tr_te_D,
    biased_rr,
    k_nn,
    aml_order,
    TrueWeightPrototypeEstimator(),
)
model.calculate_transfer_risk()
#+end_src

#+RESULTS:

Let's look at how this evolved over time, and compare it with the ITL baseline
#+begin_src jupyter-python
fig, ax = plt.subplots()
t = np.arange(model.loss_matrix_.shape[0])
plot_aml_ci(ax, model.loss_matrix_, "blue", "aml", 100)
plot_itl_ci(ax, itl_rr.losses_, "red", "itl", 100)
ax.axhline(noise_y, color="black")
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.lines.Line2D at 0x7f2b7c4bef90>
#+attr_org: :width 387
[[file:./.ob-jupyter/8c7b935cd57e943f45c73c6b94e2edce90c479e2.png]]
:END:

#+begin_src jupyter-python
zero_vs_ws = np.linalg.norm(test_ws, axis=1)
itl_vs_ws = np.linalg.norm(itl_rr.weights_ - test_ws, axis=1)
aml_w0_vs_ws = np.linalg.norm(model.w_0_[-1, :] - test_ws, axis=1)
aml_w_hat_vs_ws = np.linalg.norm(model.w_hat_[-1, :] - test_ws, axis=1)

fig, ax = plt.subplots(4, 1, sharex=True)
plot_hist_with_ci(zero_vs_ws, ax[0])
plot_hist_with_ci(itl_vs_ws, ax[1])
plot_hist_with_ci(aml_w0_vs_ws, ax[2])
plot_hist_with_ci(aml_w_hat_vs_ws, ax[3])
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 370
[[file:./.ob-jupyter/2d912703f785225795f240975cb1fbce36e1d16f.png]]
:END:

* Hypercube
:LOGBOOK:
CLOCK: [2020-05-14 Thu 15:12]--[2020-05-14 Thu 15:32] =>  0:20
CLOCK: [2020-05-13 Wed 16:14]--[2020-05-13 Wed 16:39] =>  0:25
CLOCK: [2020-05-12 Tue 20:26]--[2020-05-12 Tue 20:51] =>  0:25
CLOCK: [2020-05-12 Tue 14:41]--[2020-05-12 Tue 15:06] =>  0:25
CLOCK: [2020-05-12 Tue 14:10]--[2020-05-12 Tue 14:35] =>  0:25
:END:
All the data generated on the hypersphere.
#+begin_src jupyter-python
y_func_misspec = y_func_2nd_order

def generate_hypercube_data(
    d, k, noise_w, noise_y, k_shot, k_query, num_train_tasks=100, num_val_tasks=100, num_test_tasks=100
):
    env = HypercubeWithKVertexGaussian(d, k, 0.0)
    return generate_data(env, noise_w, noise_y, k_shot, k_query, num_train_tasks, num_val_tasks, num_test_tasks, y_func_misspec)

d = 10
k = 4
noise_w, noise_y = 0.001, 0.0
k_shot, k_query = 10, 25
n_tr, n_val, n_te = 100, 80, 100
train_tasks, train_tasks_kh, val_tasks, val_tasks_kh, test_tasks, test_tasks_kh = generate_hypercube_data(d, k, noise_w, noise_y, k_shot, k_query, n_tr, n_val, n_te)
train_datasets = form_datasets_from_tasks(train_tasks)
val_datasets = form_datasets_from_tasks(val_tasks)
test_datasets = form_datasets_from_tasks(test_tasks)

train_ws = get_task_parameters(train_tasks)
val_ws = get_task_parameters(val_tasks)
test_ws = get_task_parameters(test_tasks)
#+end_src

#+RESULTS:

Plotting the weights
#+begin_src jupyter-python
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
plot_task_weights(train_tasks, ax)
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f2b7cfd5110>
#+attr_org: :width 596
[[file:./.ob-jupyter/66e419e911c4b85e6914d43c4d4523208423ff25.png]]
:END:

Plot of the actual linear regression datasets generated. We pick 3 random tasks
which differ
#+begin_src jupyter-python
fig = plt.figure(figsize=(8, 16))
axes = [fig.add_subplot(3, 1, i, projection="3d") for i in range(1, 4)]
rand_idx = np.random.permutation(20)
for i, ax in enumerate(axes):
    task = train_tasks[rand_idx[i]]
    plot_task_lin_reg(task, ax)
    w = task["w"]
    ax.set_title("w: ({:.2f}, {:.2f})".format(w[0], w[1]))
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 460
[[file:./.ob-jupyter/f1af6642115a9487530049bf9a948a0fc0dd7514.png]]
:END:

From the above plot we can see how they differ, since we are assuming a linear
regression setup the points all lie on hyperspheres.

Now we consider how the different KH methods pick points.
#+begin_src jupyter-python
# KH on data
base_s2_D, meta_s2_D = calculate_double_gaussian_median_heuristics(
    train_datasets, n_base_subsamples=200
)
K_D = _gaussian_kernel_mmd2_matrix(
    A=train_datasets, B=train_datasets, base_s2=base_s2_D, meta_s2=meta_s2_D
)
# KH on weights
train_task_ws = get_task_parameters(train_tasks)
ws_dataset = train_task_ws[:, np.newaxis, :]
base_s2_w, meta_s2_w = calculate_double_gaussian_median_heuristics(
    ws_dataset, n_base_subsamples=200
)
K_w = _gaussian_kernel_mmd2_matrix(
    A=ws_dataset, B=ws_dataset, base_s2=base_s2_w, meta_s2=meta_s2_w
)
#+end_src

#+RESULTS:

Investigate how the kernel matrix look like.
#+begin_src jupyter-python
fig, ax = plt.subplots()
ax.imshow(K_D, vmin=0.0, vmax=1.0)
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.image.AxesImage at 0x7f2b7c6c9f10>
#+attr_org: :width 252
[[file:./.ob-jupyter/65868c4cb5799b9df3b7030c9d6562dbb230b043.png]]
:END:

#+begin_src jupyter-python
fig, ax = plt.subplots()
ax.imshow(K_w, vmin=0.0, vmax=1.0)
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.image.AxesImage at 0x7f2b7c624690>
#+attr_org: :width 252
[[file:./.ob-jupyter/447cd073a1fe5a6ca87f8461995c84c9851d0add.png]]
:END:

#+begin_src jupyter-python
fig, ax = plt.subplots()
_ = ax.hist((K_w - K_D)[np.triu_indices(n_tr, k=1)], range=(-1.0, 1.0), density=True)
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 373
[[file:./.ob-jupyter/2ccf49812aeb8863b78b7a27d66e556185e4b321.png]]
:END:

Looks pretty nice above, the weights and data dataset kernel matrix seem to find
something reasonable as the difference is not too unlike.

Running the actual kernel herding algorithms
#+begin_src jupyter-python
kh_D = KernelHerding(K_D)
kh_D.run()
kh_w = KernelHerding(K_w)
kh_w.run()
#+end_src

#+RESULTS:


#+begin_src jupyter-python
plot_kh_weights(kh_w, kh_D, train_tasks, until_t=8)
#+end_src

#+RESULTS:
:RESULTS:
: array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f2b7c352e90>,
:        <matplotlib.axes._subplots.AxesSubplot object at 0x7f2b7c30e510>,
:        <matplotlib.axes._subplots.AxesSubplot object at 0x7f2b7c2cf450>],
:       dtype=object)
[[file:./.ob-jupyter/f7b4a8281418525ffb7f774bf8553edc419d7a6e.png]]
:END:

** KNN Meta learning
:LOGBOOK:
CLOCK: [2020-05-13 Wed 17:48]--[2020-05-13 Wed 18:13] =>  0:25
:END:
We run KNN meta learning on the dataset to see and understand what works where.
We consider
- Uniform
- KH on data
- KH on weights
and we use the true weights. Alternatively, we will shrink the true weights to
see how that impacts (and possibly make it better than using the true weights).
This is due to the observed phenomenon where the ridge weights perform better
than the true weights.

Calculate the mmd distances from all the train to test tasks. These are used
later when we use the KNN algorithm
#+begin_src jupyter-python
train_datasets = form_datasets_from_tasks(train_tasks)
# Don't cheat, can only use support/train set for meta-test and meta-val
# tr_val_datasets = form_datasets_from_tasks(val_tasks, use_only_train=True)
tr_test_datasets = form_datasets_from_tasks(test_tasks, use_only_train=True)
# M_tr_val_D = np.sqrt(_mmd2_matrix(train_datasets, tr_val_datasets, base_s2_D))
M_tr_te_D = np.sqrt(_mmd2_matrix(train_datasets, tr_test_datasets, base_s2_D))
#+end_src

#+RESULTS:

We have two things to set, \(\alpha_{RR}, \alpha_{prot}\) for the regularisation
parameter in Ridge Regression and Prototypes.
#+begin_src jupyter-python
alpha_rrs = np.geomspace(1e-6, 3, 10)
alpha_prot = 0.001
#+end_src

#+RESULTS:

** ITL Ridge Reg
Evaluate ridge regression in an independent task learning setting
#+begin_src jupyter-python
rr = RidgeRegression(alpha=0.0001)
itl_rr = IndependentTaskLearning(train_tasks, rr)
itl_rr_opt_params, itl_rr_cross_val_loss = cross_validate_itl(
    itl_rr, {"alpha": alpha_rrs.tolist()}
)
itl_rr.set_params(itl_rr_opt_params)
itl_rr.tasks = test_tasks
itl_rr.calculate_transfer_risk()
#+end_src

#+RESULTS:
#+begin_example
2020-05-14 18:30:41.512 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 1e-06}
2020-05-14 18:30:41.575 | INFO     | __main__:cross_validate_itl:63 - Current loss: 3179.470771740626
2020-05-14 18:30:41.576 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 1e-06}, with loss 3179.4708
2020-05-14 18:30:41.577 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 5.244210785953468e-06}
2020-05-14 18:30:41.636 | INFO     | __main__:cross_validate_itl:63 - Current loss: 1113.934897035401
2020-05-14 18:30:41.638 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 5.244210785953468e-06}, with loss 1113.9349
2020-05-14 18:30:41.645 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 2.7501746767510743e-05}
2020-05-14 18:30:41.761 | INFO     | __main__:cross_validate_itl:63 - Current loss: 374.35266677864786
2020-05-14 18:30:41.762 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 2.7501746767510743e-05}, with loss 374.3527
2020-05-14 18:30:41.763 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.0001442249570307409}
2020-05-14 18:30:41.864 | INFO     | __main__:cross_validate_itl:63 - Current loss: 117.0119714149051
2020-05-14 18:30:41.865 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.0001442249570307409}, with loss 117.0120
2020-05-14 18:30:41.866 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.0007563460752642876}
2020-05-14 18:30:41.922 | INFO     | __main__:cross_validate_itl:63 - Current loss: 49.5932770140661
2020-05-14 18:30:41.926 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.0007563460752642876}, with loss 49.5933
2020-05-14 18:30:41.927 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.0039664382458145546}
2020-05-14 18:30:42.010 | INFO     | __main__:cross_validate_itl:63 - Current loss: 21.950437900110828
2020-05-14 18:30:42.013 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.0039664382458145546}, with loss 21.9504
2020-05-14 18:30:42.015 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.02080083823051906}
2020-05-14 18:30:42.081 | INFO     | __main__:cross_validate_itl:63 - Current loss: 11.767928268844214
2020-05-14 18:30:42.082 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.02080083823051906}, with loss 11.7679
2020-05-14 18:30:42.083 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.10908398020536153}
2020-05-14 18:30:42.162 | INFO     | __main__:cross_validate_itl:63 - Current loss: 8.173665806170547
2020-05-14 18:30:42.162 | INFO     | __main__:cross_validate_itl:68 - Best params so far {'alpha': 0.10908398020536153}, with loss 8.1737
2020-05-14 18:30:42.163 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 0.5720593855676914}
2020-05-14 18:30:42.275 | INFO     | __main__:cross_validate_itl:63 - Current loss: 8.482122372140534
2020-05-14 18:30:42.276 | INFO     | __main__:cross_validate_itl:59 - Cross validating params: {'alpha': 3.0}
2020-05-14 18:30:42.387 | INFO     | __main__:cross_validate_itl:63 - Current loss: 13.357542832168772
#+end_example

The train loss
#+begin_src jupyter-python
fig, ax = plt.subplots()
sns.distplot(itl_rr.losses_, kde=True, rug=True, ax=ax)
ax.axvline(itl_rr.losses_.mean(), color="red", linestyle="--")
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.lines.Line2D at 0x7f2b7c1a4190>
#+attr_org: :width 380
[[file:./.ob-jupyter/dfc18ae49765965979cd4efee63a895e37856b2f.png]]
:END:

** Meta-learning setup
:LOGBOOK:
CLOCK: [2020-05-13 Wed 18:49]--[2020-05-13 Wed 19:14] =>  0:25
:END:
Now we proceed to do active meta learning. We will reorder the sequences
according to KH on weights and data. As the actual meta learning algorithm we
will use the KNN on datasets and use the biased ridge regression. We will reuse
the optimal alpha from ITL with ridge reg. From here on we split into two cases,
where for the prototype estimator we use
- Ridge Regression
- True Weights

we use 3 for the number of nearest neighbours
#+begin_src jupyter-python
k_nn = 3
#+end_src

#+RESULTS:

*** Biased Ridge Reg
:LOGBOOK:
CLOCK: [2020-05-13 Wed 19:34]--[2020-05-13 Wed 19:59] =>  0:25
:END:
#+begin_src jupyter-python
aml_order = kh_D.sampled_order
biased_rr = BiasedRidgeRegression(alpha=itl_rr_opt_params["alpha"])
model = MetaKNNExperiment(
    train_tasks,
    test_tasks,
    M_tr_te_D,
    biased_rr,
    k_nn,
    aml_order,
    RidgeRegPrototypeEstimator(alpha=alpha_prot),
)
model.calculate_transfer_risk()
#+end_src

#+RESULTS:

Let's look at how this evolved over time, and compare it with the ITL baseline
#+begin_src jupyter-python
fig, ax = plt.subplots()
t = np.arange(model.loss_matrix_.shape[0])
plot_aml_ci(ax, model.loss_matrix_, "blue", "aml", n_tr)
plot_itl_ci(ax, itl_rr.losses_, "red", "itl", n_tr)
ax.axhline(noise_y, color="black")
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.lines.Line2D at 0x7f2b7c12c5d0>
#+attr_org: :width 380
[[file:./.ob-jupyter/927109b504a8fd4374e64a1fbf1e8c334c005e3b.png]]
:END:

#+begin_src jupyter-python
zero_vs_ws = np.linalg.norm(test_ws, axis=1)
itl_vs_ws = np.linalg.norm(itl_rr.weights_ - test_ws, axis=1)
aml_w0_vs_ws = np.linalg.norm(model.w_0_[-1, :] - test_ws, axis=1)
aml_w_hat_vs_ws = np.linalg.norm(model.w_hat_[-1, :] - test_ws, axis=1)

fig, ax = plt.subplots(4, 1, sharex=True)
plot_hist_with_ci(zero_vs_ws, ax[0])
plot_hist_with_ci(itl_vs_ws, ax[1])
plot_hist_with_ci(aml_w0_vs_ws, ax[2])
plot_hist_with_ci(aml_w_hat_vs_ws, ax[3])
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 374
[[file:./.ob-jupyter/683625b02ae5050ff3cdd171e4db10559decabd9.png]]
:END:

*** True weights
:LOGBOOK:
CLOCK: [2020-05-13 Wed 19:34]--[2020-05-13 Wed 19:59] =>  0:25
:END:
#+begin_src jupyter-python
aml_order = kh_D.sampled_order
biased_rr = BiasedRidgeRegression(alpha=itl_rr_opt_params["alpha"])
model = MetaKNNExperiment(
    train_tasks,
    test_tasks,
    M_tr_te_D,
    biased_rr,
    k_nn,
    aml_order,
    TrueWeightPrototypeEstimator(),
)
model.calculate_transfer_risk()
#+end_src

#+RESULTS:

Let's look at how this evolved over time, and compare it with the ITL baseline
#+begin_src jupyter-python
fig, ax = plt.subplots()
t = np.arange(model.loss_matrix_.shape[0])
plot_aml_ci(ax, model.loss_matrix_, "blue", "aml", n_tr)
plot_itl_ci(ax, itl_rr.losses_, "red", "itl", n_tr)
ax.axhline(noise_y, color="black")
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.lines.Line2D at 0x7f2b7c761090>
#+attr_org: :width 370
[[file:./.ob-jupyter/82eea2dae5257f9b1d926333aa0d4e0806a63db9.png]]
:END:

#+begin_src jupyter-python
zero_vs_ws = np.linalg.norm(test_ws, axis=1)
itl_vs_ws = np.linalg.norm(itl_rr.weights_ - test_ws, axis=1)
aml_w0_vs_ws = np.linalg.norm(model.w_0_[-1, :] - test_ws, axis=1)
aml_w_hat_vs_ws = np.linalg.norm(model.w_hat_[-1, :] - test_ws, axis=1)

fig, ax = plt.subplots(4, 1, sharex=True)
plot_hist_with_ci(zero_vs_ws, ax[0])
plot_hist_with_ci(itl_vs_ws, ax[1])
plot_hist_with_ci(aml_w0_vs_ws, ax[2])
plot_hist_with_ci(aml_w_hat_vs_ws, ax[3])
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 370
[[file:./.ob-jupyter/f9ac3b8fcb8b9de62d8e9fcca6f297b00537ca8d.png]]
:END:
